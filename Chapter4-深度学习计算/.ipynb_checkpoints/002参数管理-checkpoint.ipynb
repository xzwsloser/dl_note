{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcbdfaf-7200-4b3a-8647-895e4296619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbcaf0-e7e7-4959-88c2-9d35d337d092",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "参数管理的内容包含: \n",
    "- 访问参数, 用于调试、诊断和可视化\n",
    "- 参数初始化\n",
    "- 在不同模型组件之间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8c7800-08e9-45bc-b938-afd6773242ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0991],\n",
       "        [0.0864]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168c047-b225-45a0-af53-614887e0328c",
   "metadata": {},
   "source": [
    "## 参数访问\n",
    "可以访问已有模型的参数, 使用 `nn.Sequential`定义模型的时候, 可以通过索引来访问模型的任意层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a444bdb-8092-4b58-a1ad-ab953d29e4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1140,  0.2452, -0.3135, -0.0166],\n",
      "        [-0.0059, -0.3440,  0.1467, -0.0842],\n",
      "        [ 0.1440, -0.1320,  0.4097,  0.0152],\n",
      "        [ 0.0424, -0.4933, -0.2836, -0.3300],\n",
      "        [ 0.2844, -0.2255, -0.0505,  0.1970],\n",
      "        [ 0.4932,  0.4995,  0.3552, -0.1307],\n",
      "        [ 0.3465,  0.0710,  0.0840, -0.1203],\n",
      "        [-0.2880, -0.4392, -0.4636, -0.3366]])), ('bias', tensor([ 0.3584,  0.0277, -0.4997,  0.1030,  0.0103,  0.3437, -0.1254, -0.3330]))])\n"
     ]
    }
   ],
   "source": [
    "# 访问一个模型的全部参数\n",
    "print(net[0].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8ead5-50a3-45e1-b853-601b24e56c2a",
   "metadata": {},
   "source": [
    "### 目标参数\n",
    "每一个参数都表示为一个参数类(`nn.parameter.Parameter`)的一个实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefc2fb0-e1f9-4ee0-9475-297f2b8af60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2667], requires_grad=True)\n",
      "tensor([0.2667])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5735fa28-09d8-4311-9dbd-358d0bb7ecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时没有使用反向传播, 所以相关参数的梯度为 None\n",
    "net[2].weight.grad == None, net[2].bias.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba22aa5-803f-47ed-801f-b41b487a2afb",
   "metadata": {},
   "source": [
    "### 一次性访问所有参数\n",
    "当需要对所有参数执行操作的时候, 逐个访问参数可能比较麻烦, 处理更加复杂的块的时候, 情况会更加复杂, 此时需要遍历整个树来提取每一个子树的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "681532b5-5c4f-45e2-837f-c8564717decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 1. 访问第一层的所有参数\n",
    "# 注意 net[0] 和 net 都是 nn.Module 类型\n",
    "# 注意其中的 * 表示分散列表中的元素类似于 ...\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "# 2. 访问网络中的所有参数\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93bda73-2889-49ea-a1b3-8d2982978d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2667])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 利用索引访问\n",
    "net.state_dict()['2.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadbfd1-6105-46f3-ade0-e22f54dfb3d3",
   "metadata": {},
   "source": [
    "### 从嵌套块收集参数\n",
    "可以使用 `nn.Module` 对象的 `add_module` 方法来添加重复的块, 从而完全块的嵌套"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238b5d4e-b042-4f37-922b-81450bcd8b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3080],\n",
       "        [-0.3080]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f436c1b8-ee70-4b50-a498-0c18af2d7605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block 0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 网络结构如下\n",
    "rgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737d512c-ec7c-4121-b942-80b09a911b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4021, -0.4630,  0.3853, -0.4227,  0.0936, -0.0953, -0.1855,  0.2166])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回第一个块, 第二个子块的第一层\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754364b6-b2bc-4d59-9b88-a7210d77df6f",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "`PyTorch`中提供默认随机初始化, 也允许我们创建自定义初始化的方法, 默认情况下 `PyTorch`会更具一个范围均匀地初始化权重和偏置矩阵, 这一个范围是根据输入和输出维度计算的, `PyTorch` 中的 `nn.init` 模块提供了多种预置初始化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a526cde-0e7f-42e3-9842-46e1fa82fa62",
   "metadata": {},
   "source": [
    "### 内置初始化\n",
    "使用 `nn.init` 模块中的各种方法, 比如 `nn.init.normal_` 或者 `nn.init.zeros_` 可可以初始化对应的参数, 结合 `net.apply(fn)` 方法, 这一个方法可以把 `fn` 作用在 `net` 的所有子模块中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d4e1f70-2d03-489c-9312-334385f44ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0134, -0.0050, -0.0047,  0.0031]), tensor(0.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高斯分布: normal_, 置0: zeros_\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd70efd9-f63c-460b-9cf3-0c4e1a211676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化为常数 constant_\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data,net[2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48a41f0-d15b-4afe-a620-101ffbb641fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0261,  0.5972,  0.0540, -0.1318],\n",
      "        [ 0.0822,  0.0775,  0.4442, -0.0087],\n",
      "        [ 0.5582, -0.1029, -0.6108,  0.3070],\n",
      "        [-0.5713, -0.4307,  0.6260,  0.6819],\n",
      "        [ 0.3809,  0.2615, -0.5368, -0.1770],\n",
      "        [ 0.6146, -0.3754,  0.5280,  0.4743],\n",
      "        [-0.6857,  0.3639, -0.6416, -0.1110],\n",
      "        [ 0.3743, -0.4182,  0.2518, -0.2293]])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 xavier 初始化方法进行初始化, 主要是 nn.Module 对象就可以初始化\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c520a-8ead-4bbb-8ba6-190d05eefcbe",
   "metadata": {},
   "source": [
    "### 自定义初始化\n",
    "使用一样的初始化方法 + `net.apply` 即可, 假设利用如下初始化方法:\n",
    "$$\n",
    "w \\sim \n",
    "\\begin{cases} \n",
    "U(5, 10) & \\text{可能性 } \\frac{1}{4} \\\\\n",
    "0 & \\text{可能性 } \\frac{1}{2} \\\\\n",
    "U(-10, -5) & \\text{可能性 } \\frac{1}{4} \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa87f467-47f3-486f-945d-5b261395d58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5319,  5.5679,  9.8325, -5.2827],\n",
       "        [-0.0000,  5.1967, -6.1572,  0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        # 类似于 bool 型索引, 使用矩阵代替元素\n",
    "        m.weight.data *= (m.weight.data.abs() >= 5)\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74402bd0-9db0-4a28-a98b-9ad69e244735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1000.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接设置参数\n",
    "net[0].weight.data[:] = 100\n",
    "net[0].weight.data[0, 0] = 1000\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70458088-5dd7-472a-abce-5dec073eee98",
   "metadata": {},
   "source": [
    "## 参数绑定\n",
    "为了在多个层之间共享参数, 只需要定义一个共享层, 之后利用它的参数来设置另一个层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ef4bebb-47d3-4cce-b9a5-41435346d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                   shared, nn.ReLU(),\n",
    "                   shared, nn.ReLU(),\n",
    "                   nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 使用列表/ndarray/tensor 代替元素\n",
    "print(net[2].weight.data == net[4].weight.data) # 注意 bool 型数组/索引的使用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
