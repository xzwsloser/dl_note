{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece50083-d106-418e-9608-e6c47a403cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c17669-a5c8-4859-bb4b-d231d560dcc7",
   "metadata": {},
   "source": [
    "# 批量规范化\n",
    "> 批量规范化是一种可以加速深度网络收敛的有效技术, 接后之后的残差块, 批量规范化可以使得训练 100 层以上的网络成为可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a4094-9eb8-49bc-9341-f91eeec8774d",
   "metadata": {},
   "source": [
    "## 训练深度网络\n",
    "批量规范化本质上是对于数据偏移的消除\n",
    "\n",
    "训练深度神经网络中可能出现的问题如下:\n",
    "1. 数据预处理方式可能对于最终结果产生巨大影响, 比如在使用 `MLP` 预测房价的时候, 需要表转化输入特征, 使得平均值为 `0`, 方差为 `1` $x \\rightarrow \\frac {x - \\mu}{\\sigma}$\n",
    "2. 对于典型多层感知机或者卷积神经网络, 训练过程中, 中间层的变量可能具有更大的变化范围, 随着时间的推移, 模型参数随着训练更新而变化, 并且这一种偏移会影响网络的收敛\n",
    "3. 更深的网络更加复杂, 容易过拟合\n",
    "\n",
    "批量规范化应用于单个可选层(或者所有层), 原理: 在每一次训练迭代中, 首先需要规范化输入, 也就是 $x \\rightarrow \\frac {x - \\mu}{\\sigma}$, 之后需要使用比例系数和比例偏移, 并且注意只有使用足够大的小批量, 批量规范化这一种方法才是有效并且稳定的 ; 但是使用批量规范化的时候, 批量大小的选择可能更加重要\n",
    "\n",
    "从形式上来说: 使用 $\\mathbf{x} \\in \\mathcal{B}$ 表示来自一个小批量 $\\mathcal{B}$ 的输入, 批量规范化根据如下表达式转换 $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}}}{\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}} + \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "其中 $\\hat{\\boldsymbol{\\mu}}$ 为小批量 $\\mathcal{B}$的样本呢均值, $\\hat{\\boldsymbol{\\sigma}}$ 为样本方差, 同时包含拉伸参数($\\mathbf{\\gamma}$)以及偏移参数 $\\mathbf{\\beta}$, 都是需要学习的参数\n",
    "\n",
    "在训练过程中, 批量规范化可以将每一层主动矩阵, 并且重新重新调整为给定的平均值和大小, 从形式上来看, 二者的计算方式如下:\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x},\n",
    "\\\\\n",
    "\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\left( \\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}} \\right)^2 + \\epsilon. \n",
    "$$\n",
    "其中方差加上一个偏移防止方差为 `0` 的时候出错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2afa3-e974-4c79-90d9-0d1b1dfc6fe6",
   "metadata": {},
   "source": [
    "## 批量规范化层\n",
    "## 全连接层\n",
    "使用批量规范化的全连接层的输出和计算详情如下:\n",
    "$$\n",
    "\\mathbf{h} = \\phi(\\text{BN}(\\mathbf{W}\\mathbf{x} + \\mathbf{b})).\n",
    "$$\n",
    "## 卷积层\n",
    "对于卷积层, 样本批量化发生在卷积层之后以及非线性激活函数之前, 需要对于这些通道的每一个输出执行批量规范化, 每一个通道都有自己的拉伸以及偏移系数, 这两个参数都是标量(平移不变性), 假设批量中含有 $m$ 个样本, 输出大小为 $p \\times q$, 那么对于卷积层, 我们在每一个输出通道的 $m \\times p \\times q$ 个元素中同时执行每个批量规范化\n",
    "### 预测过程\n",
    "批量规范化在训练过程和预测过程中的模式不同,将训练得到的模型用于预测的时候, 不需要样本均值中的噪声以及微批量上轨迹每一个小批量产生的样本方差, 一种常用的方法是通过移动平均估计估算整个训练数据集的样本均值和方差, 并且在预测时使用他吗呢得到确定的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5c6ad-1557-4f96-a9dd-b12c3ccf971b",
   "metadata": {},
   "source": [
    "## BatchNorm层实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0172963-e83e-4721-9ff3-f2d76763b58b",
   "metadata": {},
   "source": [
    "### 造轮子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b202a974-0af3-4b22-96bc-cf23cdabd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式\n",
    "    if not torch.is_grad_enabled():\n",
    "        # 如果是预测模式, 直接输出预测值\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0) \n",
    "        else:\n",
    "            # 使用二维卷积层\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            # 对于每一个批量以及通道求解均值\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1 - momentum) * var\n",
    "    Y = gamma * X_hat + beta # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba9995-ad9f-4117-ac01-81ccf9799985",
   "metadata": {},
   "source": [
    "注意到其中 `momentum` 用于利用当前批次方差来更新全局的统计量(全局平均数以及全局移动方差), 调用 `model.eval()` 之后, `Batch Norm` 层会使用训练阶段累积的全局统计量来表转化输入, 而不是实时接计算批次统计量, 其中 momentum 表示当前批次对于全局的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f8bdc0-781d-4076-8a63-bc5cf2dbeb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm 层\n",
    "class BatchNorm(nn.Module):\n",
    "    # num_features: 完全连接层的输出数量或者卷积层的输出通道数\n",
    "    # num_dims: 2 -> 全连接层, 4 -> 卷积层\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)  # 针对于每一个特征\n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean, \n",
    "            self.moving_var, eps=1e-5, momentum=0.9\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f4825-d526-4da8-851c-fca27ae8fd6a",
   "metadata": {},
   "source": [
    "### 简洁实现\n",
    "> 使用 LeNet 为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4a48c5-db13-4918-8500-4a66a0e54119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意这里的维数为:  维度数量 * 形状, 后面的形状表示维数\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
