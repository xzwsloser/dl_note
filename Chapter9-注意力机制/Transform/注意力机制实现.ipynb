{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573b2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3430549",
   "metadata": {},
   "source": [
    "# 注意力机制实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe71c0",
   "metadata": {},
   "source": [
    "## self-attention实现\n",
    "> 参考: https://www.bilibili.com/video/BV19YbFeHETz?spm_id_from=333.788.videopod.sections&vd_source=b419802666550a8f77628730aa29c06b\n",
    "\n",
    "`Attention`计算公式:\n",
    "$$\n",
    "Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db45c5a",
   "metadata": {},
   "source": [
    "### self-attention实现(第一层)\n",
    "> 只考虑公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e40e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention 实现\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 728) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Q,K,V 权重矩阵 hidden_dim*hidden_dim\n",
    "        self.query_proj = nn.Linear(hidden_dim,hidden_dim, bias=False)\n",
    "        self.key_proj = nn.Linear(hidden_dim,hidden_dim, bias=False)\n",
    "        self.value_proj = nn.Linear(hidden_dim,hidden_dim, bias=False)\n",
    "    def forward(self, X):\n",
    "        # X.shape -> (batch_size, seq, dim)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "        # Q,K,V -> (batch_size, seq, dim)\n",
    "        # 计算注意力评分函数\n",
    "        attention_value = torch.bmm(Q, K.permute(0, 2, 1))\n",
    "        # 计算 softmax\n",
    "        # attention_weights.shape -> (batch_size, seq, seq)\n",
    "        self._attention_weights = torch.softmax(attention_value/math.sqrt(self.hidden_dim), dim=-1)\n",
    "        Y = torch.bmm(self._attention_weights, V)\n",
    "        return Y\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed16832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights = \t tensor([[[0.4867, 0.5133],\n",
      "         [0.4865, 0.5135]],\n",
      "\n",
      "        [[0.4910, 0.5090],\n",
      "         [0.4893, 0.5107]],\n",
      "\n",
      "        [[0.4978, 0.5022],\n",
      "         [0.5054, 0.4946]]], grad_fn=<SoftmaxBackward0>)\n",
      "Y = \t tensor([[[-0.0028, -0.1970,  0.2460,  0.4847],\n",
      "         [-0.0028, -0.1970,  0.2461,  0.4847]],\n",
      "\n",
      "        [[ 0.2811, -0.1148, -0.0239,  0.1229],\n",
      "         [ 0.2816, -0.1148, -0.0239,  0.1228]],\n",
      "\n",
      "        [[ 0.1856, -0.1672,  0.2208,  0.2608],\n",
      "         [ 0.1864, -0.1670,  0.2212,  0.2580]]], grad_fn=<BmmBackward0>)\n",
      "X.shape:  torch.Size([3, 2, 4]) \t Y.shape:  torch.Size([3, 2, 4]) \tattention_weights:  torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 测试 self_attention\n",
    "X = torch.rand((3, 2, 4))\n",
    "self_attr_net1 = SelfAttentionV1(hidden_dim=4)\n",
    "Y = self_attr_net1(X)\n",
    "print('attention_weights = \\t', self_attr_net1.attention_weights)\n",
    "print('Y = \\t', Y)\n",
    "print('X.shape: ', X.shape, '\\t Y.shape: ', Y.shape, '\\tattention_weights: ', self_attr_net1.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acb967",
   "metadata": {},
   "source": [
    "### Self-Attention实现(第二层)\n",
    "> QKV矩阵优化, 使用一个矩阵表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79ce6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, dim: int = 512) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Linear(dim, dim*3)\n",
    "    def forward(self, X):\n",
    "        # X.shape -> (batch_size, seq, dim)\n",
    "        QKV = self.proj(X)\n",
    "        # QKV.shape -> (batch_size, seq, dim*3)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=2)\n",
    "        # Q,K,V.shape -> (batch_size, seq, dim*3)\n",
    "        attention_value = torch.bmm(Q,K.permute(0, 2, 1)) / math.sqrt(self.dim)\n",
    "        # attention_value.shape -> (batch_size, seq, seq)\n",
    "        self._attention_weights = torch.softmax(attention_value,dim=-1)\n",
    "        Y = torch.bmm(self._attention_weights, V)\n",
    "        return Y\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b930cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  tensor([[[0.8800, 0.3124, 0.4686, 0.3256],\n",
      "         [0.1465, 0.7232, 0.9188, 0.6566]],\n",
      "\n",
      "        [[0.0145, 0.5094, 0.8818, 0.4162],\n",
      "         [0.1500, 0.8649, 0.2281, 0.2649]],\n",
      "\n",
      "        [[0.2055, 0.6865, 0.6145, 0.5852],\n",
      "         [0.5847, 0.8555, 0.4184, 0.1247]]])\n",
      "attention_weights =  tensor([[[0.5123, 0.4877],\n",
      "         [0.5264, 0.4736]],\n",
      "\n",
      "        [[0.4727, 0.5273],\n",
      "         [0.4830, 0.5170]],\n",
      "\n",
      "        [[0.4786, 0.5214],\n",
      "         [0.4957, 0.5043]]], grad_fn=<SoftmaxBackward0>)\n",
      "Y =  tensor([[[ 0.3297,  0.2641, -0.2786, -0.5755],\n",
      "         [ 0.3288,  0.2607, -0.2785, -0.5748]],\n",
      "\n",
      "        [[ 0.2304,  0.3561, -0.3570, -0.4942],\n",
      "         [ 0.2295,  0.3578, -0.3562, -0.4936]],\n",
      "\n",
      "        [[ 0.3708,  0.2249, -0.3171, -0.5494],\n",
      "         [ 0.3687,  0.2293, -0.3175, -0.5511]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 2, 4)\n",
    "self_attr_net = SelfAttentionV2(4)\n",
    "Y = self_attr_net(X)\n",
    "print('X = ', X)\n",
    "print('attention_weights = ', self_attr_net.attention_weights)\n",
    "print('Y = ', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fa71a",
   "metadata": {},
   "source": [
    "### Self-Attention实现(第三层)\n",
    "优化内容如下:\n",
    "1. 加入 `Dropout` 层(`Softmax`之后)\n",
    "2. 加入 `attention_mask`\n",
    "3. 加入 `output`矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f93ec1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, dim: int=512, dropout_rate:float =0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Linear(dim, dim*3)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X.shape -> (batch_size, seq, dim)\n",
    "        QKV = self.proj(X)\n",
    "        Q,K,V = torch.split(QKV, self.dim, dim=-1)\n",
    "        # 计算 attention_weights\n",
    "        attention_value = Q @ K.permute(0, 2, 1) / math.sqrt(self.dim)\n",
    "        # mask 操作\n",
    "        if attention_mask is not None:\n",
    "            attention_value = attention_value.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-1e20\")\n",
    "            )\n",
    "        attention_weights = torch.softmax(attention_value,dim=-1)\n",
    "        # 注意 nn.Dropout 的位置\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = attention_weights @ V\n",
    "        output = self.output_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf9b805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape =  torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 4, 2)\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "])\n",
    "# 注意 mask 需要和 attention_value 的形状一样 -> (batch_size, seq, seq)\n",
    "# (batch_size, seq) -> (batch_size, seq, seq)\n",
    "attention_mask = attention_mask.unsqueeze(1).repeat(1, 4, 1)\n",
    "self_attr_net = SelfAttentionV3(dim=2)\n",
    "Y = self_attr_net(X, attention_mask)\n",
    "print('Y.shape = ', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d27fbe",
   "metadata": {},
   "source": [
    "### Self-Attention面试写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20cab2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionInterView(nn.Module):\n",
    "    def __init__(self, dim: int = 512, dropout_rate: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # Q,K,V 矩阵\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "        # Dropout 层\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        # Output 映射\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "        self._attention_weights = None\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X.shape -> (batch_size, seq, dim)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "        # Q,K,V shape -> (batch_size, seq, dim)\n",
    "        attention_value = Q @ K.permute(0, 2, 1) / math.sqrt(self.dim)\n",
    "        # attention_value.shape -> (batch_size, seq, seq)\n",
    "        # mask 操作\n",
    "        if attention_mask is not None:\n",
    "            attention_value = attention_value.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "        # softmax 操作\n",
    "        self._attention_weights = self.attention_dropout(\n",
    "            torch.softmax(attention_value, dim=-1)\n",
    "        )\n",
    "        # 计算 output \n",
    "        output = self._attention_weights @ V\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c23edcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  tensor([[[-0.7204, -0.2448],\n",
      "         [-0.8157,  0.1936],\n",
      "         [-0.8218, -0.0176],\n",
      "         [-0.8137,  0.1958]],\n",
      "\n",
      "        [[-0.7438,  0.0593],\n",
      "         [-0.7438,  0.0593],\n",
      "         [-0.6876, -0.1786],\n",
      "         [-0.7438,  0.0593]],\n",
      "\n",
      "        [[-0.9652,  0.1250],\n",
      "         [-0.9652,  0.1250],\n",
      "         [-0.9652,  0.1250],\n",
      "         [-0.9652,  0.1250]]], grad_fn=<ViewBackward0>)\n",
      "Y.shape =  torch.Size([3, 4, 2])\n",
      "attention_weights =  tensor([[[0.0000, 0.3630, 0.0000, 0.0000],\n",
      "         [0.3486, 0.3919, 0.3706, 0.0000],\n",
      "         [0.0000, 0.3886, 0.3748, 0.0000],\n",
      "         [0.3516, 0.3676, 0.3919, 0.0000]],\n",
      "\n",
      "        [[0.5519, 0.5592, 0.0000, 0.0000],\n",
      "         [0.5524, 0.5587, 0.0000, 0.0000],\n",
      "         [0.0000, 0.5596, 0.0000, 0.0000],\n",
      "         [0.5521, 0.5590, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "         [1.1111, 0.0000, 0.0000, 0.0000],\n",
      "         [1.1111, 0.0000, 0.0000, 0.0000],\n",
      "         [1.1111, 0.0000, 0.0000, 0.0000]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "])\n",
    "mask = mask.unsqueeze(1).repeat(1, 4, 1)\n",
    "self_attr_net = SelfAttentionInterView(2)\n",
    "Y = self_attr_net(X, mask)\n",
    "print('Y = ', Y)\n",
    "print('Y.shape = ', Y.shape)\n",
    "print('attention_weights = ', self_attr_net.attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced74de",
   "metadata": {},
   "source": [
    "## Multi-Head Attention 实现\n",
    "多头注意力结构如下:\n",
    "![img1](img/2025-07-23_18-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18144145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力机制实现\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim) # 这里其实是 num_heads * head_dim\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X.shape -> (batch_size, seq, hidden_dim)\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "        # Q,K,V.shape -> (batch_size, seq, hidden_dim)\n",
    "        # Q,K,V 分层 -> (batch_size, num_heads, seq, head_dim)\n",
    "        Q = Q.reshape(batch_size, seq_len, -1, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, seq_len, -1, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, seq_len, -1, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q,K,V.shape -> (batch_size, num_heads, seq, head_dim)\n",
    "        attention_value = Q @ K.permute(0, 1, 3, 2) / math.sqrt(self.head_dim)\n",
    "        # attention_value 形状 -> (batch_size, num_heads, seq, seq)\n",
    "        if attention_mask is not None:\n",
    "            attention_value = attention_value.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-1e20\")\n",
    "            )\n",
    "        # softmax 操作 + Dropout 操作\n",
    "        attention_weights = torch.softmax(attention_value, dim=-1)\n",
    "        print('attention_weights.shape = ', attention_weights.shape)\n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "        # 输出数据\n",
    "        # attention_weights -> (batch_size, num_heads, seq, seq)\n",
    "        # V                 -> (batch_size, num_heads, seq, num_hiddens)\n",
    "        # (batch_size, num_heads, seq, num_hiddens) -> (batch_size, seq, num_hiddens)\n",
    "        output = (attention_weights @ V).permute(0, 2, 1, 3).reshape(batch_size, seq_len, -1) \n",
    "        output = self.output_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fff65714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights.shape =  torch.Size([3, 8, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试多头注意力机制\n",
    "X = torch.rand(3, 2, 128)\n",
    "# (2, 2) -> (3, 8, 2, 2)\n",
    "mask = torch.tensor([\n",
    "    [1, 1],\n",
    "    [1, 0]\n",
    "])\n",
    "mask = mask.unsqueeze(0).unsqueeze(1).repeat(3, 8, 1, 1)\n",
    "multi_attention = MultiHeadAttention(128, 8)\n",
    "Y = multi_attention(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492634c6",
   "metadata": {},
   "source": [
    "## 总结\n",
    "无论是对于 `Self-Attention`还是`Multi-Head Attention`, 注意可以采用的优化策略都有:\n",
    "1. 使用一个大矩阵来包含变换矩阵\n",
    "2. 注意矩阵变换过程中的维度变换(相邻的维度可以合并, 同时一个维度可以拆分为相邻的两个维度的和, 使用 `view`变换需要操作连续内存空间, 但是使用 `reshape`不需要, 使用 `transpose`进行矩阵两个维度的交换, 使用 `permute` 进行维度的变换)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
